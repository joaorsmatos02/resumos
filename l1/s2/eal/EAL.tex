\documentclass[10pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[portuguese]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{lmodern}
\usepackage{amsfonts}
\usepackage{amssymb}
\begin{document}

\chapter{Espaços vetoriais}

\section{Subespaços vetoriais}

\subsection{Definição}
F é um subespaço vetorial de $\mathbb{R}^n$ se:
\begin{itemize}
\item $F \subseteq \mathbb{R}^n$;
\item $0_n = (0, 0, ..., 0) \in F$;
\item Se $u, v \in F$, então $u + v \in F$\\($F$ é fechado para a adição)
\item Se $\alpha$ é um número real e $v \in F$, então $\alpha v \in F$.\\($F$ é fechado para a multiplicação escalar)a
\end{itemize}

\subsection{Combinações lineares}
Um conjunto de vetores diz-se linearmente dependente se pelo menos 1 deles for combinação linear dos outros, i.e. se for obtivel através de uma combinação de somas e multiplicações escalares dos outros. Para verificar a dependência linear colocam-se os vetores nas linhas de uma matriz e de seguida em forma de escada, se tiver pelo menos uma linha nula são dependentes, caso contrario são independentes.

\subsection{Bases}
Uma sequência de vetores é uma base de $V$ se:
\begin{itemize}
\item Os vetores $v_1, ..., v_P$ são linearmente independentes
\item $< v_1, ..., v_p > = V$
\end{itemize}
A base canónica de $\mathbb{R}^n$ é:
$$
((1, 0, ..., 0), (0, 1, ..., 0), ..., (0, ..., 0, 1))
$$
A dimensão de uma base é o número de vetores l.i. que lhe pertencem, $dim(\mathbb{R}^n) = n$

\subsection{Espaço das linhas e colunas}
Seja $A$ uma matriz $m \times n$. Chamamos espaço das linhas de $A$ e representamos por $L(A)$ ao subespaço de $\mathbb{R}^m$ gerado pelas linhas de $A$.\\
Analogamente, chamamos espaço das colunas de $A$ e representamos por $C(A)$ ao subespaço de $\mathbb{R}^n$ gerado pelas colunas de $A$\\
\\
Sejam $A \in M_{m\times n}$ e $B \in M_{m\times 1}$. O sistema $AX = B$ só é possível se $B \in C(A)$.

\subsection{Núcleo}
O núcleo de uma matriz $A \in M_{m\times n}$, denotado por $N(A)$ é o conjunto das soluções de $AX=0$.\\
\\
$$
dim(N(A)) = n - r(A)
$$\\
\\
Se $A$ é uma matriz quadrada de ordem $n$ então é equivalente dizer:
\begin{itemize}
\item $N(A) = 0_n$
\item $A$ é invertível
\item $r(A) = n$
\item $dim(A) \neq 0$
\item $L(A) = \mathbb{R}^n$
\end{itemize}

\subsection{Sistema de equações cartesianas}
Dado um subespaço vetorial com $dim = n$, o procedimento para obter o seu sistema de equações cartesianas é o seguinte:
\begin{itemize}
\item Construir uma matriz com os $n$ vetores nas suas colunas e as variáveis na sua parte ampliada;
\item Colocar a matriz em forma de escada;
\item Verificar, de acordo com as variáveis, quais os seus valores que tornam o sistema possível (para que $r(A) = r(A|B))$;
\item Retirar o sistema de equações 
\end{itemize} 

\subsection{Sistemas de Cramer}
Dizemos que um sistema de equaçõoes lineares $S$ é um sistema de
Cramer se:
\begin{itemize}
\item O número de equações de $S$ = o número de incógnitas de $S$;
\item O determinante da matriz simples de $S$ é diferente de zero.
\end{itemize}

\subsection{Regra de Cramer}
Sejam $A \in Mn$ e $B \in M_{n \times 1}$ tais que $A X = B$ é sistema de Cramer. Se $(\alpha_1, ..., \alpha_n)$ é a sua solução, então, para
todo o $i \in (1, ..., n)$,
$$
\alpha_i = \frac{1}{det(A)}det(A_i),
$$
sendo $A_i$ a matriz que se obtém de $A$ substituindo a coluna $i$ por
$B$.

\chapter{Aplicações}

\section{Injetividade e sobrejetividade}
\begin{itemize}
\item $f$ é injetiva se, para quaisquer $w, u \in E$, se $w \neq u$, então $f(w) \neq f(u)$.
\item $f$ é sobrejetiva se, para qualquer $v \in V$, existe $w \in E$ tal que $f(w) = v$.
\item $f$ é bijetiva de é injetiva e sobrejetiva.
\end{itemize}

\section{Aplicações lineares}
\subsection{Definição}
$f$ é uma aplicação linear se:
\begin{itemize}
\item $f(u+w) = f(u) + f(w)$
\item $f(au) = af(u)$
\end{itemize}

\subsection{Propriedades}
Se $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ é uma aplicação linear, então:
\begin{itemize}
\item $f(0_n) = 0_m$;
\item $f(-u) = -f(u)$
\item Para quaisquer $a_1, ..., a_t \in \mathbb{R}$ e $u_1, ..., u_t \in \mathbb{R}^n$,$$
f(a_1u_1 + ... + a_tu_t) = a_1f(u_1) + ... + a_tf(u_t)$$
\end{itemize}

\subsection{Matriz canónica}
Considere-se a aplicação:
$$
f(x_1, x_2) = (2x_1+x_2, 3x_1, x_2)
$$
A matriz canónica de f é:
$$
M(f) =  \begin{bmatrix} 2 & 1 \\ 3 & 0 \\ 0 & 1 \end{bmatrix} 
$$

\subsection{Composição de aplicações}
Sejam $g$ e $f$ aplicações lineares.
\begin{itemize}
\item $g \circ f$ é aplicação linear
\item $M( g \circ f) = M(g)M(f)$
\end{itemize}

\subsection{Núcleo}
O núcleo de uma aplicação linear é o subconjunto:
$$
Nuc f = (y_1, ..., y_n) \in \mathbb{R}^n : f(y_1, ..., y_n) = 0_m
$$

Se $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ e $A = M(f)$ tem-se:
\begin{itemize}
\item $Nuc (f) = N(A)$
\item $Nuc (f)$ é subespaço de $\mathbb{R}^n$
\item $Im (f) = C(A)$
\item $Im (f)$ é subespaço de $\mathbb{R}^m$
\item $f$ é injetiva só se $Nuc (f) = 0_n$
\end{itemize}
Logo, $dim (Im (f)) = r(A)$ e $dim (Nuc (f)) = n - r(A)$


\chapter{Valores e vetores próprios}

\section{Valores próprios}
\subsection{Definição}
Diz-se que um número real $\lambda$ é valor próprio de $f$ se existe $v \in \mathbb{R}^n$ tal que $v \neq 0_n$ e $f(v) = \lambda v$.

\subsection{Determinar valores próprios}
Um número real $\lambda$ é valor próprio de $A$ se e só se $det (A - \lambda I_n) = 0$. Logo para determinar os valores próprios de $A$  devemos resolver a equação característica de $A$, $det(A-xI_n)=0$, sendo que $det(A-xI_n)$ é o polinómio característico de $A$.

\section{Vetores próprios}
\subsection{Definição}
Diz-se que um vetor $v$ é vetor próprio de $f$ se $v \neq 0_n$ e existe um número real $\lambda$ tal que $f(v) = \lambda v$.

\subsection{Determinar vetores próprios}
Os vetores próprios de uma matriz $A$ associados a $\lambda$ são as soluções não nulas do sistema homogéneo $(A-\lambda I_n)X = 0$. Logo, os vetores próprios de $A$ associados a $\lambda$ são os vetores não nulos de $N(A - \lambda I_n)$.

\subsection{Subespaço próprio}
A $N(A-\lambda I_n)$ chamamos subespaço próprio de $A$ associado a $\lambda$. Denotamos este subespaço por $E_\lambda$. À dimensão de $E_\lambda$ chamamos multiplicidade geométrica de $\lambda$ e denotamos $mg(\lambda)$.

\subsection{Retirar subespaço}
Para retirar um subespaço de uma matriz em forma de escada é necessário:
\begin{itemize}
\item Retirar as linhas nulas;
\item Escrever uma das variáveis em função das outras. Ex: Retiramos a linha $[1 -3 1]$, temos então que $x - 3y + z = 0$ e retiramos que $x = 3y - z$;
\item Substituir a variável escolhida pela função;
\item Separar o vetor em vários, cada um contendo apenas uma variável;
\item Retirar as variáveis de dentro do vetor (para multiplicação);
\item Remover as variáveis e escrever a base do subespaço.
\end{itemize}

\section{Diagonalização}
\subsection{Definição}
Uma dada matriz $A$ diz-se diagonalizável se existe uma outra matriz $P$ tal que $P^{-1}AP$ é uma matriz diagonal. Neste caso, $P$ é uma matriz diagonalizante de $A$.

\subsection{Propriedades}
Sejam $A, P \in M_n$:
\begin{itemize}
\item A matriz $P$ é uma matriz diagonalizante de $A$ se e só se as colunas
de $P$ s˜ao vetores próprios de $A$ e s˜ao linearmente independentes.
\item A matriz $A$ é diagonalizável só se tiver $n$ vetores próprios linearmente independentes.
\item Se $A$ tem $n$ valores próprios, então é diagonalizável.
\end{itemize}

\subsection{Determinar matriz diagonalizante}
Para determinar uma matriz $P$ diagonalizante de $A$ procedemos da seguinte forma:
\begin{itemize}
\item Para cada valor próprio $\lambda$ da matriz $A$ determinamos uma base do subespaço próprio $E_\lambda$.
\item Consideramos o conjunto $(z_1, ..., z_n)$ formado por todos os vetores das bases encontradas.
\item Constrói-se uma matriz $P$ tendo $z_1, ..., z_n$ como colunas
\end{itemize}


\chapter{Produto interno}
\section{Definições}
Sejam $u$ e $v$ dois vetores de $\mathbb{R}^n$
$$
\cos (u, v) = \frac{u \cdot v}{\|u\| \cdot \|v\|}
$$
$$
proj_au= a \cdot \left(\frac{u \cdot a}{\|a\|^2}\right)
$$
\vspace{0.5cm}
Em $\mathbb{R}^3$:
\vspace{0.5cm}
$$
u \times v = \left( \begin {vmatrix} u_2 & u_3 \\ v_2 & v_3 \end {vmatrix} ,
- \begin {vmatrix} u_1 & u_3 \\ v_1 & v_3 \end {vmatrix} ,
\begin {vmatrix} u_1 & u_2 \\ v_1 & v_2 \end {vmatrix}  \right)\\
$$
Nota: $\|u \times v\|$ é igual á a área do paralelogramo definido por u e v.
$$
(u \times v) \cdot w = \begin{vmatrix}
u_1 & u_2 & u_3 \\
v_1 & v_2 & v_3 \\
w_1 & w_2 & w_3
\end{vmatrix}
$$
Nota: $\|(u \times v) \cdot w\|$ é igual á ao volume do paralelepípedo definido por u, v e w.

\section{Bases ortogonais e ortonormadas}
Dois vetores $u$ e $v$ dizem-se ortogonais se $u \cdot v = 0$. Uma base diz-se ortogonal se for formada apenas por vetores ortogonais e ortonormada se todos os vetores têm norma 1.

\subsection{Ortogonalização}
Seja $F$ um subespaço de $\mathbb{R}^n$ e seja $(v_1, ..., v_p)$ uma base de $F$. Se $(z_1, ..., z_p)$ é uma base ortogonal de $F$, então:

\begin{align}
z_1 &= v_1\\
z_2 &= v_2 - proj_{z_1} v_2\\
z_3 &= v_3 - proj_{z_1} v_3 - proj_{z_2} v_3\\
\vdots
\end{align}

\subsection{Ortonormalização}
Para obter uma base ortonormada de $F$, deve-se dividir cada vetor da sua base ortogonal pela sua norma. Seguindo o exemplo acima, $z_1$ iria dividir-se por $\|z_1\|$, $z_2$ por $\|z_2\|$, etc.

\section{Matrizes ortogonais}
\subsection{Definição}
Uma matriz $A$ é ortogonal se é invertível e $A^{-1} = A^T$

\end{document}

